<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>blog app</title>
    <link rel="stylesheet" href="./style.css">
    <link rel="stylesheet" href="utility.css">

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">

<body>

    <!-- nav bar start -->
    <nav>
        <div class="fluid">
            <ul class="container">
                <li class="logo t-d"><a href="./index.html">My Personal Blog</a> </li>
                <li><a href="./signup.html">Log In</a></li>
            </ul>
        </div>
    </nav>
    <!-- nav bar end -->
    <a href="./index.html" class="color t-d">
        <h1 class="container g-morning bold ">
            <span class="boldm">&lt;</span>Back to All blog
        </h1>
    </a>
    <!-- card  -->
    <div class="d-flex">
        <div class=" card1 w-70p m-l-10p">

            <div class="d-flex">
                <img src="./assets/profile 2.png" alt="img" class="img-w-h">

                <div class="int-p">
                    <h5>Introducing Whisper</h5>
                    <p>Elon Musk - August 17th, 2023</p>
                </div>
            </div>
            <div class="container mt-10">
                <p>Whisper is an automatic speech recognition (ASR) system trained on 680,000 hours of multilingual and
                    multitask supervised data collected from the web. We show that the use of such a large and diverse
                    dataset leads to improved robustness to accents, background noise and technical language. Moreover,
                    it
                    enables transcription in multiple languages, as well as translation from those languages into
                    English.
                    We are open-sourcing models and inference code to serve as a foundation for building useful
                    applications
                    and for further research on robust speech processing.</p>

            </div>

        </div>
        <!-- side image  -->
        <div class="">
            <a href="https://openai.com/" class=" color-b bold ">
                <p class=" t-r-align">elon@openai.com</p>
            </a>
            <h2 class="color bold t-r-align">Elon Musk</h2>
            <img src="./assets/profile 1.png" alt="img" class="side-img ">
        </div>
    </div>
    <!-- chat gpt section  -->
    <div class=" card1 w-70p m-l-10p">

        <div class="d-flex">
            <img src="./assets/profile 2.png" alt="img" class="img-w-h">

            <div class="int-p">
                <h5>
                    Introducing ChatGPT</h5>
                <p>Elon Musk - November 23rd, 2022</p>
            </div>
        </div>
        <div class="container mt-10">
            <p>We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes
                it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises,
                and reject inappropriate requests. <br>
                We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes
                it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises,
                and reject inappropriate requests. <br><br>Methods: <br>
                We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as
                <u> InstructGPT,</u> but with slight differences in the data collection setup. We trained an initial
                model using
                supervised fine-tuning: human AI trainers provided conversations in which they played both sides—the
                user and an AI assistant. We gave the trainers access to model-written suggestions to help them compose
                their responses. We mixed this new dialogue dataset with the InstructGPT dataset, which we transformed
                into a dialogue format. <br>
                To create a reward model for reinforcement learning, we needed to collect comparison data, which
                consisted of two or more model responses ranked by quality. To collect this data, we took conversations
                that AI trainers had with the chatbot. We randomly selected a model-written message, sampled several
                alternative completions, and had AI trainers rank them. Using these reward models, we can fine-tune the
                model using <u>Proximal Policy Optimization.</u> We performed several iterations of this process.
            </p>
        </div>

    </div>
    <script src="https://www.gstatic.com/firebasejs/9.6.7/firebase-app-compat.js"></script>
    <script src="https://www.gstatic.com/firebasejs/9.6.7/firebase-auth-compat.js"></script>
    <script src="https://www.gstatic.com/firebasejs/9.6.7/firebase-firestore-compat.js"></script>
    <script src="./app.js"></script>
</body>

</html>